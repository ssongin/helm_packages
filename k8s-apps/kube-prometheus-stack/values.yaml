kube-prometheus-stack:
  prometheus:
    # ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#prometheusspec
    prometheusSpec:
      # ref: https://github.com/prometheus-operator/prometheus-operator/blob/release-0.44/pkg/prometheus/promcfg.go#L180-L183
      scrapeInterval: 30s

      retention: 3d

      # Watch all PrometheusRules in the cluster.
      ruleNamespaceSelector:
        matchLabels: {}
      ruleSelector:
        matchLabels: {}

      # Watch all ServiceMonitors in the cluster.
      serviceMonitorNamespaceSelector:
        matchLabels: {}
      serviceMonitorSelector:
        matchLabels: {}

      # Watch all PodMonitors in the cluster.
      podMonitorSelector:
        matchLabels: {}
      podMonitorNamespaceSelector:
        matchLabels: {}

      resources:
        requests:
          cpu: 300m
          memory: 100Mi
        limits:
          cpu: 600m
          memory: 500Mi

      # ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/user-guides/storage.md
      storageSpec:
        volumeClaimTemplate:
          spec:
            accessModes: ["ReadWriteOnce"]
            resources:
              requests:
                storage: 15Gi

      thanos:
        {}
        # objectStorageConfig:
        #   key: objstore.yml
        #   name: thanos-objstore-secret
    ingress:
      enabled: true
      ingressClassName: traefik

      annotations:
        cert-manager.io/cluster-issuer: selfsigned-cluster-issuer
        external-dns.alpha.kubernetes.io/hostname: &host prometheus.homelab.net
        external-dns.alpha.kubernetes.io/target: home.homelab.net
        traefik.ingress.kubernetes.io/router.entrypoints: websecure
        external-dns.alpha.kubernetes.io/cloudflare-proxied: "false"
        gethomepage.dev/enabled: "true"
        gethomepage.dev/name: Prometheus
        gethomepage.dev/description: Monitoring system and time series database
        gethomepage.dev/group: Monitoring
        gethomepage.dev/icon: https://raw.githubusercontent.com/prometheus/prometheus/refs/heads/main/documentation/images/prometheus-logo.svg
        # nginx.ingress.kubernetes.io/auth-url: https://$host/oauth2/auth
        # nginx.ingress.kubernetes.io/auth-signin: https://$host/oauth2/start?rd=$escaped_request_uri
      hosts:
        - *host
      tls:
        - secretName: prometheus-general-tls
          hosts:
            - *host

    thanosService:
      enabled: false

    thanosServiceMonitor:
      enabled: false

  # ==============================================================================
  # Grafana
  # Default values: https://github.com/grafana/helm-charts/blob/main/charts/grafana/values.yaml
  # ==============================================================================

  grafana:
    enabled: false
    replicas: 1
    defaultDashboardsTimezone: Europe/Vilnius

    # Required as long as Grafana's persistent volume is ReadWriteOnce.
    # During a rolling update, the new Grafana pod would not be able to start
    # while the old pod still holds the volume.
    deploymentStrategy:
      type: Recreate

    serviceMonitor:
      scrapeTimeout: 10s

    ingress:
      enabled: true
      ingressClassName: traefik

      annotations:
        cert-manager.io/cluster-issuer: selfsigned-cluster-issuer
        external-dns.alpha.kubernetes.io/hostname: &host grafana.homelab.net
        external-dns.alpha.kubernetes.io/target: home.homelab.net
        traefik.ingress.kubernetes.io/router.entrypoints: websecure
        external-dns.alpha.kubernetes.io/cloudflare-proxied: "false"
        gethomepage.dev/enabled: "true"
        gethomepage.dev/name: Grafana
        gethomepage.dev/description: "The open platform for beautiful analytics and monitoring."
        gethomepage.dev/group: Monitoring
        gethomepage.dev/icon: https://raw.githubusercontent.com/grafana/grafana/refs/heads/main/public/img/grafana_icon.svg

        cert-manager.io/issue-temporary-certificate: "true"
        nginx.ingress.kubernetes.io/auth-url: https://$host/oauth2/auth
        nginx.ingress.kubernetes.io/auth-signin: https://$host/oauth2/start?rd=$escaped_request_uri
        nginx.ingress.kubernetes.io/auth-response-headers: X-Auth-Request-Email
      hosts:
        - *host
      tls:
        - secretName: grafana-general-tls
          hosts:
            - *host

    # The oAuth2 proxy handles authentication, not Grafana.
    grafana.ini:
      # auth:
      #   disable_login_form: true
      #   signout_redirect_url: "https://grafana.terence.cloud/oauth2/sign_out"
      # auth.basic:
      #   enabled: false
      # auth.proxy:
      #   enabled: true
      #   header_name: X-Auth-Request-Email
      #   header_property: email
      #   auto_sign_up: true
      # users:
      #   allow_sign_up: false
      #   auto_assign_org: true
      #   auto_assign_org_role: Admin

    resources:
      requests:
        cpu: 100m
        memory: 150Mi
      limits:
        cpu: 250m
        memory: 400Mi

    # Organise dashboards by provider, with the provider's name as the key.
    dashboards:
      default: # The "default" provider is defined below.
        traefik-ingress-controller:
          url: https://github.com/traefik/traefik/blob/master/contrib/grafana/traefik-kubernetes.json
        # nginx-ingress-controller:
        #   url: https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v0.48.1/deploy/grafana/dashboards/nginx.json
        # ingress-request-performance:
        #   url: https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v0.48.1/deploy/grafana/dashboards/request-handling-performance.json
        cloudnativepg:
          gnetId: 20417
          datasource: Prometheus
        # longhorn:
        #   gnetId: 16888
        #   datasource: Prometheus
      extra:
        {}
        # pihole-overview:
        #   gnetId: 10176
        #   datasource: Prometheus
        # argocd-overview:
        #   gnetId: 14584
        #   datasource: Prometheus
        # cert-manager-overview:
        #   gnetId: 11001
        #   datasource: Prometheus
        # external-dns-overview:
        #   gnetId: 15038
        #   datasource: Prometheus
        # blackbox-exporter-overview:
        #   gnetId: 5345
        #   datasource: Prometheus
        # thanos-overview:
        #   gnetId: 12937
        #   datasource: Thanos

    # Configure dashboard providers.
    # ref: http://docs.grafana.org/administration/provisioning/#dashboards
    # `path` must be /var/lib/grafana/dashboards/<provider_name>
    dashboardProviders:
      dashboardproviders.yaml:
        apiVersion: 1
        providers:
          - name: default
            orgId: 1
            folder: ""
            type: file
            disableDeletion: false
            editable: true
            allowUiUpdates: true
            options:
              path: /var/lib/grafana/dashboards/default
          - name: "extra"
            orgId: 1
            folder: "Extra"
            type: file
            disableDeletion: true
            editable: true
            allowUiUpdates: true
            options:
              path: /var/lib/grafana/dashboards/extra

    persistence:
      enabled: true
      size: 10Gi
      accessModes:
        - ReadWriteOnce
      finalizers:
        - kubernetes.io/pvc-protection

    datasources:
      datasources.yaml:
        apiVersion: 1
        datasources:
          - name: Prometheus
            type: prometheus
            url: http://kube-prometheus-stack-prometheus:9090
            access: proxy
          - name: Loki
            type: loki
            url: http://loki-gateway.loki
            access: proxy

    plugins:
      - grafana-piechart-panel
  # ==============================================================================
  # Alertmanager
  # ref: https://prometheus.io/docs/alerting/alertmanager/
  # ==============================================================================

  alertmanager:
    enabled: false
    alertmanagerSpec:
      # useExistingSecret: true
      secrets:
        - telegram-bot-token
      storage:
        volumeClaimTemplate:
          spec:
            accessModes: ["ReadWriteOnce"]
            resources:
              requests:
                storage: 1Gi

    config:
      receivers:
        - name: empty
        - name: telegram
          telegram_configs:
            - send_resolved: true
              bot_token_file: /etc/alertmanager/secrets/telegram-bot-token/token
              chat_id: -1002235030838
              message: |
                {{ define "__custom_text_alert_list" }}{{ range . }}
                ---
                ü™™ <b>{{ .Labels.alertname }}</b>
                {{- if .Annotations.summary }}
                üìù {{ .Annotations.summary }}{{ end }}
                {{- if .Annotations.description }}
                üìñ {{ .Annotations.description }}{{ end }}
                üè∑ Labels:
                {{ range .Labels.SortedPairs }}  <i>{{ .Name }}</i>: <code>{{ .Value }}</code>
                {{ end }}{{ end }}
                üõ† <a href="https://grafana.terence.cloud">Grafana</a> üõ†
                {{ end }}

                {{ if gt (len .Alerts.Firing) 0 }}
                üî• Alerts Firing üî•
                {{ template "__custom_text_alert_list" .Alerts.Firing }}
                {{ end }}
                {{ if gt (len .Alerts.Resolved) 0 }}
                ‚úÖ Alerts Resolved ‚úÖ
                {{ template "__custom_text_alert_list" .Alerts.Resolved }}
                {{ end }}
      route:
        group_by: ["alertname"]
        group_wait: 0s
        receiver: telegram
        routes:
          - matchers:
              - alertname=~"CNPGClusterHA.*|KubeCPUOvercommit|KubeMemoryOvercommit|Watchdog"
            receiver: empty

    ingress:
      enabled: false
      ingressClassName: nginx
      annotations:
        cert-manager.io/cluster-issuer: letsencrypt
        external-dns.alpha.kubernetes.io/target: home.terence.cloud
        nginx.ingress.kubernetes.io/auth-url: "https://$host/oauth2/auth"
        nginx.ingress.kubernetes.io/auth-signin: "https://$host/oauth2/start?rd=$escaped_request_uri"
      hosts:
        - alertmanager.terence.cloud
      paths:
        - /
      pathType: ImplementationSpecific
      tls:
        - secretName: alertmanager-tls
          hosts:
            - alertmanager.terence.cloud

  kubeControllerManager:
    enabled: false

  kubeEtcd:
    enabled: false

  kubeScheduler:
    enabled: false

  kubeProxy:
    enabled: false
